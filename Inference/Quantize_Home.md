# Model Quantize 

### to read and Summarize:

1. GGUF量化
2. Achieving FP32 Accuracy for INT8 Inference Using Quantization Aware Training with NVIDIA TensorRT:[Nvidia_Doc](https://developer.nvidia.com/blog/achieving-fp32-accuracy-for-int8-inference-using-quantization-aware-training-with-tensorrt/)

3. [Quantize Llama models with GGUF and llama.cpp](https://towardsdatascience.com/quantize-llama-models-with-ggml-and-llama-cpp-3612dfbcc172)
4. GPTQ: Post-Training Quantization for GPT Models
5. AWQ：Activation-aware Weight Quantization


## TBD...