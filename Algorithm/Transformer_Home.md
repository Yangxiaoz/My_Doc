# Transformer & LLM基础理论


## 一、资料收录


| 名称 | 简介 | 源自 | 链接|
|:---:|:----: |:---: |:---:| 
| Attention is all you need | Transformer架构论文原文|arXiv|[Link](https://arxiv.org/abs/1706.03762)|
| Attention is all you need | 李沐：Transformer论文精读|Bilibili|[Link](https://www.bilibili.com/video/BV1pu411o7BE/?spm_id_from=333.999.0.0&vd_source=ffd358de576192fb36c26e0aa712f76f)|
|The Ilustrated GPT-2 |关于GPT2原理解释博客|Web| [Link](https://jalammar.github.io/illustrated-gpt2/)|
| Llama 2: Open Foundation and Fine-Tuned Chat Models | llama2论文原文|arXiv|[Link](https://arxiv.org/abs/2307.09288)|
| Llama 2详解 | 关于Llama原理解释博客|知乎|[Link](https://zhuanlan.zhihu.com/p/649756898)|
| The Llama 3 Herd of Models | llama3论文原文|arXiv|[Link](https://arxiv.org/abs/2407.21783)|



## 二、学习记录与总结


| 序号 | 简介       | 地址 | 状态|
|:---:|:----- |:--- |:---:| 
| 1 | 前言---Transformer与LLM的概念解释与理解|[Link](./Transformer_0.md)| ✅Done|
| 2 |理解Attention---从人类思考过程理解Attention算子|[Link](./Transformer_1.md)| ✅Done|
| 3 | 图解LLM推理完整流程---Attention算子|[Link](./Transformer_2.md)| ✅Done|
| 4 | LLM推理过程量化分析：weight、kvcache、flops|[Link](./Transformer_3.md)| 🚧Unfinished|








